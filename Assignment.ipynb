{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e930bb25",
      "metadata": {
        "id": "e930bb25"
      },
      "source": [
        "**Fine-Tuning a Large Language Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e35b68",
      "metadata": {
        "id": "70e35b68"
      },
      "source": [
        "### 1. Set up Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d113f3b9",
      "metadata": {
        "id": "d113f3b9"
      },
      "outputs": [],
      "source": [
        "pip install torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "676e6529",
      "metadata": {
        "id": "676e6529"
      },
      "outputs": [],
      "source": [
        "pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fddfaae5",
      "metadata": {
        "id": "fddfaae5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OH-HZBXf5YDl",
      "metadata": {
        "id": "OH-HZBXf5YDl"
      },
      "source": [
        "### 2. Exploring the dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5247b70a",
      "metadata": {
        "id": "5247b70a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('knkarthick/dialogsum')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "854c131e",
      "metadata": {
        "id": "854c131e"
      },
      "source": [
        "Print several dialogues with their baseline summaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65a8e4d3",
      "metadata": {
        "id": "65a8e4d3"
      },
      "outputs": [],
      "source": [
        "example_indices = [0, 42, 800]\n",
        "dash_line = '-' * 100\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    print(dash_line)\n",
        "    print('Example', i + 1)\n",
        "    print(dash_line)\n",
        "    print('INPUT DIALOGUE:')\n",
        "    print(dataset['test'][index]['dialogue'])\n",
        "    print(dash_line)\n",
        "    print('BASELINE HUMAN SUMMARY:')\n",
        "    print(dataset['test'][index]['summary'])\n",
        "    print(dash_line)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdd15699",
      "metadata": {
        "id": "bdd15699"
      },
      "source": [
        "### 3. Summarizing dialogues without the Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f575c6f",
      "metadata": {
        "id": "4f575c6f"
      },
      "source": [
        "**Loading** Flan-T5-large model and the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cde7777e",
      "metadata": {
        "id": "cde7777e"
      },
      "outputs": [],
      "source": [
        "model_name = 'google/flan-t5-large'\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1acc5c4c",
      "metadata": {
        "id": "1acc5c4c"
      },
      "outputs": [],
      "source": [
        "\n",
        "gen_text = []\n",
        "for i, index in enumerate(example_indices):\n",
        "    inputs = tokenizer(dataset['test'][index]['dialogue'], return_tensors='pt', truncation=True)\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 50)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f56c1847",
      "metadata": {
        "id": "f56c1847"
      },
      "source": [
        "The model's outputs show logical coherence, yet they lack clear task understanding. Rather than completing the intended objective, the system frequently generates continuation dialogue. Strategic prompt design can address this issue."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d357a5e",
      "metadata": {
        "id": "3d357a5e"
      },
      "source": [
        "### 4. Creating Dialogue Summaries Through Instructional Prompting\n",
        "To guide a model toward executing a particular function (such as dialogue summarization), transform the conversation into a task-specific instruction. This approach is commonly known as zero-shot inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce562836",
      "metadata": {
        "id": "ce562836"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt = \"Summarize this conversation:\\n\"\n",
        "for i, index in enumerate(example_indices):\n",
        "    ip = prompt + dataset['test'][index]['dialogue']\n",
        "    inputs = tokenizer(ip, return_tensors='pt')\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 50)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3f84813",
      "metadata": {
        "id": "b3f84813"
      },
      "source": [
        "This shows significant improvement! However, the model continues to miss the subtle details and contextual nuances present in the conversations.Retry"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd35a918",
      "metadata": {
        "id": "fd35a918"
      },
      "source": [
        "Try modifying the prompt structure and observe its impact on the results. Notice whether the model's responses differ when you conclude the prompt with nothing versus adding Summary:  at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tzzxiWsM7Mzd",
      "metadata": {
        "id": "tzzxiWsM7Mzd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "prompt = \"Summarize this conversation:\\n\"\n",
        "end_prompt = \"\\n Summary: \"\n",
        "for i, index in enumerate(example_indices):\n",
        "    ip = prompt + dataset['test'][index]['dialogue'] + end_prompt\n",
        "    inputs = tokenizer(ip, return_tensors='pt')\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 50)\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aca21d4c",
      "metadata": {
        "id": "aca21d4c"
      },
      "source": [
        "### 5. Dialogue Summarization Using Few-Shot Inference\n",
        "\n",
        "Few-shot inference involves supplying a language model with multiple example prompt-response combinations that demonstrate your desired task before presenting the actual query you need answered. This technique, known as \"in-context learning,\" primes the model to comprehend and execute your particular objective."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1df2552a",
      "metadata": {
        "id": "1df2552a"
      },
      "source": [
        "Build a function that takes a list of `in_context_example_indexes`, generates a prompt with the examples, then at the end appends the prompt that you want the model to complete (`test_example_index`). Use the same Flan-T5 prompt template from Section 3. Make sure to separate between the examples with `\"\\n\\n\\n\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130b8c38",
      "metadata": {
        "id": "130b8c38"
      },
      "outputs": [],
      "source": [
        "def make_prompt(in_context_example_indices, test_example_index):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e968a7e1",
      "metadata": {
        "id": "e968a7e1"
      },
      "outputs": [],
      "source": [
        "in_context_example_indices = [0, 10, 20]\n",
        "test_example_index = 800\n",
        "\n",
        "few_shot_prompt = make_prompt(in_context_example_indices, test_example_index)\n",
        "print(few_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb1afcfc",
      "metadata": {
        "id": "bb1afcfc"
      },
      "source": [
        "Now pass this prompt to the model perform a few shot inference:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e588bf0c",
      "metadata": {
        "id": "e588bf0c"
      },
      "source": [
        "**Exercise:** Experiment with the few-shot inferencing:\n",
        "- Choose different dialogues - change the indices in the `in_context_example_indices` list and `test_example_index` value.\n",
        "- Change the number of examples. Be sure to stay within the model's 512 context length, however.\n",
        "\n",
        "How well does few-shot inference work with other examples?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd99b9cf",
      "metadata": {
        "id": "fd99b9cf"
      },
      "source": [
        "### 6. Adjusting Generation Settings for Model Inference\n",
        "\n",
        "The generate() method's configuration settings can be modified to produce varied LLM outputs. Previously, you've only specified max_new_tokens=50, which controls the token generation limit. The GenerationConfig class offers an efficient approach to managing these settings. Enabling do_sample = True unlocks different decoding methods that affect token selection from the complete vocabulary's probability distribution. You can fine-tune results by modifying temperature along with additional parameters (like top_k and top_p). For a comprehensive parameter list, refer to the Hugging Face Generation documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2a66a1",
      "metadata": {
        "id": "6a2a66a1"
      },
      "source": [
        "**Exercise:** Change the configuration parameters to investigate their influence on the output. Analyze your results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fzZMfTDEhBpy",
      "metadata": {
        "id": "fzZMfTDEhBpy"
      },
      "source": [
        "### 7. Fine-tuning the Model on DialogSum Dataset\n",
        "\n",
        "After exploring prompt engineering techniques, we can further improve performance by fine-tuning the model on the DialogSum dataset. This section demonstrates how to set up and execute the fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gm23DUA_fRPq",
      "metadata": {
        "id": "gm23DUA_fRPq"
      },
      "outputs": [],
      "source": [
        "# Import additional libraries needed for fine-tuning\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oj9HUMJlhEJs",
      "metadata": {
        "id": "oj9HUMJlhEJs"
      },
      "source": [
        "First, we need to prepare the dataset for fine-tuning by tokenizing the inputs and targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lvuD5sg7gS1o",
      "metadata": {
        "id": "lvuD5sg7gS1o"
      },
      "outputs": [],
      "source": [
        "# Load a smaller model for fine-tuning\n",
        "model_name = 'google/flan-t5-base'  # Using base model for faster fine-tuning\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the dataset\n",
        "def preprocess_function(examples):\n",
        "    # Create instruction prompts\n",
        "    inputs = [\"Summarize this conversation:\\n\" + dialogue for dialogue in examples['dialogue']]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "\n",
        "    # Tokenize targets\n",
        "    labels = tokenizer(examples['summary'], max_length=128, truncation=True)\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7AWROV-1gcP-",
      "metadata": {
        "id": "7AWROV-1gcP-"
      },
      "source": [
        "Now we'll set up the training arguments and initialize the trainer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Jr1DKUN3gXdS",
      "metadata": {
        "id": "Jr1DKUN3gXdS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=0.001,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    logging_steps=500,\n",
        "\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=original_model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B4Ky8Ro7ghY6",
      "metadata": {
        "id": "B4Ky8Ro7ghY6"
      },
      "source": [
        "Now we can start the fine-tuning process. This might take a while depending on your hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JEn0rYxzgkGc",
      "metadata": {
        "id": "JEn0rYxzgkGc"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J9vSIJuEgmSx",
      "metadata": {
        "id": "J9vSIJuEgmSx"
      },
      "source": [
        "After training, we'll save the fine-tuned model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3QVq_OHPgon5",
      "metadata": {
        "id": "3QVq_OHPgon5"
      },
      "outputs": [],
      "source": [
        "model_path = './flan-t5-base-dialogsum-checkpoint'\n",
        "\n",
        "original_model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jNhhSVntgq_X",
      "metadata": {
        "id": "jNhhSVntgq_X"
      },
      "source": [
        "Finally, let's load the fine-tuned model and test it on some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aUf9EU8gs4U",
      "metadata": {
        "id": "9aUf9EU8gs4U"
      },
      "outputs": [],
      "source": [
        "instruct_model = AutoModelForSeq2SeqLM.from_pretrained('./flan-t5-base-dialogsum-checkpoint',\n",
        "                                                       torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G1zpROnrg8mL",
      "metadata": {
        "id": "G1zpROnrg8mL"
      },
      "outputs": [],
      "source": [
        "# Let's test our fine-tuned model on the same examples we used before\n",
        "for i, index in enumerate(example_indices):\n",
        "    prompt = \"Summarize this conversation:\\n\" + dataset['test'][index]['dialogue']\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    outputs = instruct_model.generate(**inputs, max_new_tokens=50)\n",
        "    print(dash_line)\n",
        "    print(f'Example {i+1}')\n",
        "    print(dash_line)\n",
        "    print('ORIGINAL DIALOGUE:')\n",
        "    print(dataset['test'][index]['dialogue'][:200] + '...')\n",
        "    print(dash_line)\n",
        "    print('HUMAN SUMMARY:')\n",
        "    print(dataset['test'][index]['summary'])\n",
        "    print(dash_line)\n",
        "    print('FINE-TUNED MODEL SUMMARY:')\n",
        "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "    print(dash_line)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DQYeXdzl79ts",
      "metadata": {
        "id": "DQYeXdzl79ts"
      },
      "outputs": [],
      "source": [
        "# Install ROUGE metric dependencies\n",
        "!pip install rouge_score -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LzTGxnaa8f0D",
      "metadata": {
        "id": "LzTGxnaa8f0D"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"No GPU detected, using CPU instead.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vpnokTji7-_6",
      "metadata": {
        "id": "vpnokTji7-_6"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Initialize a ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Check GPU availability\n",
        "cuda_available = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if cuda_available else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load a fresh instance of the baseline model with GPU support\n",
        "baseline_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base')\n",
        "if cuda_available:\n",
        "    baseline_model = baseline_model.to(device).half()  # Use half precision for GPU efficiency\n",
        "\n",
        "# Ensure fine-tuned model is on the same device (GPU)\n",
        "if cuda_available:\n",
        "    instruct_model = instruct_model.to(device)\n",
        "\n",
        "print(\"Comparing summaries before and after fine-tuning...\\n\")\n",
        "\n",
        "# Compare results for each example\n",
        "for i, index in enumerate(example_indices):\n",
        "    prompt = \"Summarize this conversation:\\n\" + dataset['test'][index]['dialogue']\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "    # Move inputs to same device as models (GPU)\n",
        "    if cuda_available:\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate summaries with both models\n",
        "    with torch.no_grad():\n",
        "        baseline_outputs = baseline_model.generate(**inputs, max_new_tokens=50)\n",
        "        finetuned_outputs = instruct_model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "    # Decode the outputs\n",
        "    baseline_summary = tokenizer.decode(baseline_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_summary = tokenizer.decode(finetuned_outputs[0], skip_special_tokens=True)\n",
        "    human_summary = dataset['test'][index]['summary']\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    baseline_scores = scorer.score(human_summary, baseline_summary)\n",
        "    finetuned_scores = scorer.score(human_summary, finetuned_summary)\n",
        "\n",
        "    # Print the results with ROUGE scores\n",
        "    print(dash_line)\n",
        "    print(f'Example {i+1}')\n",
        "    print(dash_line)\n",
        "    print('ORIGINAL DIALOGUE:')\n",
        "    print(dataset['test'][index]['dialogue'][:200] + '...')\n",
        "    print(dash_line)\n",
        "    print('HUMAN SUMMARY:')\n",
        "    print(human_summary)\n",
        "    print(dash_line)\n",
        "    print('BEFORE FINE-TUNING (BASELINE) SUMMARY:')\n",
        "    print(baseline_summary)\n",
        "    print(\"\\nROUGE Scores (Baseline vs Human):\")\n",
        "    print(f\"ROUGE-1: {baseline_scores['rouge1'].fmeasure:.4f}\")\n",
        "    print(f\"ROUGE-2: {baseline_scores['rouge2'].fmeasure:.4f}\")\n",
        "    print(f\"ROUGE-L: {baseline_scores['rougeL'].fmeasure:.4f}\")\n",
        "    print(dash_line)\n",
        "    print('AFTER FINE-TUNING SUMMARY:')\n",
        "    print(finetuned_summary)\n",
        "    print(\"\\nROUGE Scores (Fine-tuned vs Human):\")\n",
        "    print(f\"ROUGE-1: {finetuned_scores['rouge1'].fmeasure:.4f}\")\n",
        "    print(f\"ROUGE-2: {finetuned_scores['rouge2'].fmeasure:.4f}\")\n",
        "    print(f\"ROUGE-L: {finetuned_scores['rougeL'].fmeasure:.4f}\")\n",
        "    print(dash_line)\n",
        "    print()\n",
        "\n",
        "# Calculate average ROUGE scores across all examples\n",
        "all_indices = example_indices\n",
        "baseline_rouge1 = 0.0\n",
        "baseline_rouge2 = 0.0\n",
        "baseline_rougeL = 0.0\n",
        "finetuned_rouge1 = 0.0\n",
        "finetuned_rouge2 = 0.0\n",
        "finetuned_rougeL = 0.0\n",
        "\n",
        "for index in all_indices:\n",
        "    prompt = \"Summarize this conversation:\\n\" + dataset['test'][index]['dialogue']\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "    # Move inputs to same device as models (GPU)\n",
        "    if cuda_available:\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate summaries with both models on GPU\n",
        "    with torch.no_grad():\n",
        "        baseline_outputs = baseline_model.generate(**inputs, max_new_tokens=50)\n",
        "        finetuned_outputs = instruct_model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "    # Decode the outputs\n",
        "    baseline_summary = tokenizer.decode(baseline_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_summary = tokenizer.decode(finetuned_outputs[0], skip_special_tokens=True)\n",
        "    human_summary = dataset['test'][index]['summary']\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    baseline_scores = scorer.score(human_summary, baseline_summary)\n",
        "    finetuned_scores = scorer.score(human_summary, finetuned_summary)\n",
        "\n",
        "    baseline_rouge1 += baseline_scores['rouge1'].fmeasure\n",
        "    baseline_rouge2 += baseline_scores['rouge2'].fmeasure\n",
        "    baseline_rougeL += baseline_scores['rougeL'].fmeasure\n",
        "    finetuned_rouge1 += finetuned_scores['rouge1'].fmeasure\n",
        "    finetuned_rouge2 += finetuned_scores['rouge2'].fmeasure\n",
        "    finetuned_rougeL += finetuned_scores['rougeL'].fmeasure\n",
        "\n",
        "# Calculate averages\n",
        "n_examples = len(all_indices)\n",
        "baseline_rouge1 /= n_examples\n",
        "baseline_rouge2 /= n_examples\n",
        "baseline_rougeL /= n_examples\n",
        "finetuned_rouge1 /= n_examples\n",
        "finetuned_rouge2 /= n_examples\n",
        "finetuned_rougeL /= n_examples\n",
        "\n",
        "# Print average scores\n",
        "print(dash_line)\n",
        "print(\"AVERAGE ROUGE SCORES ACROSS ALL EXAMPLES\")\n",
        "print(dash_line)\n",
        "print(\"Baseline Model:\")\n",
        "print(f\"ROUGE-1: {baseline_rouge1:.4f}\")\n",
        "print(f\"ROUGE-2: {baseline_rouge2:.4f}\")\n",
        "print(f\"ROUGE-L: {baseline_rougeL:.4f}\")\n",
        "print(\"\\nFine-tuned Model:\")\n",
        "print(f\"ROUGE-1: {finetuned_rouge1:.4f}\")\n",
        "print(f\"ROUGE-2: {finetuned_rouge2:.4f}\")\n",
        "print(f\"ROUGE-L: {finetuned_rougeL:.4f}\")\n",
        "print(\"\\nImprovement:\")\n",
        "print(f\"ROUGE-1: {finetuned_rouge1 - baseline_rouge1:.4f} ({(finetuned_rouge1 - baseline_rouge1) / baseline_rouge1 * 100:.2f}%)\")\n",
        "print(f\"ROUGE-2: {finetuned_rouge2 - baseline_rouge2:.4f} ({(finetuned_rouge2 - baseline_rouge2) / baseline_rouge2 * 100:.2f}%)\")\n",
        "print(f\"ROUGE-L: {finetuned_rougeL - baseline_rougeL:.4f} ({(finetuned_rougeL - baseline_rougeL) / baseline_rougeL * 100:.2f}%)\")\n",
        "print(dash_line)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_oaZ9yVw8Fgv",
      "metadata": {
        "id": "_oaZ9yVw8Fgv"
      },
      "source": [
        "### 8. Conclusion\n",
        "\n",
        "This notebook examined various prompt design strategies for summarizing dialogues:\n",
        "\n",
        "1. Basic zero-shot generation without task guidance\n",
        "2. Zero-shot generation using instructional prompts\n",
        "3. Few-shot generation incorporating sample demonstrations\n",
        "4. Optimization of generation parameters\n",
        "5. Model fine-tuning using the DialogSum dataset\n",
        "\n",
        "Each method presents distinct advantages and drawbacks. Prompt engineering can enhance output quality without altering the underlying model, while fine-tuning enables deeper task-specific learning that typically produces superior results. That said, fine-tuning demands greater computational power and processing time than prompt engineering methods.\n",
        "Combining these strategies - applying prompt engineering with a fine-tuned model - generally delivers optimal performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L9zi4HHkOgHv",
      "metadata": {
        "id": "L9zi4HHkOgHv"
      },
      "source": [
        "## 9. Error Analysis\n",
        "\n",
        "With the fine-tuned model complete and baseline comparisons made, we can now perform an in-depth error examination. This analysis will reveal the specific mistake categories our model produces and uncover systematic patterns in its failure cases.Retry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SyXXAeppOh2D",
      "metadata": {
        "id": "SyXXAeppOh2D"
      },
      "outputs": [],
      "source": [
        "# Let's analyze more examples to identify error patterns\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random.seed(42)\n",
        "\n",
        "# Select 20 random examples for analysis\n",
        "analysis_indices = random.sample(range(len(dataset['test'])), 20)\n",
        "\n",
        "# Create a dataframe to store the results\n",
        "error_analysis_data = []\n",
        "\n",
        "# Error categories\n",
        "error_categories = {\n",
        "    'missing_key_info': 'Missing key information from the dialogue',\n",
        "    'hallucination': 'Adding details not present in the dialogue',\n",
        "    'context_misunderstanding': 'Misunderstanding the context or relationships',\n",
        "    'length_issues': 'Summary too long or too short',\n",
        "    'entity_confusion': 'Confusion about entities or speakers',\n",
        "    'focus_problems': 'Missing the main point of the conversation'\n",
        "}\n",
        "\n",
        "# Function to manually categorize errors (simplified for demonstration)\n",
        "def categorize_errors(human_summary, model_summary):\n",
        "    # This is a simplified categorization based on some heuristics\n",
        "    # Real-world analysis might involve human evaluators\n",
        "    errors = []\n",
        "\n",
        "    # Basic length check\n",
        "    if len(model_summary.split()) < 5 or len(model_summary.split()) > 2*len(human_summary.split()):\n",
        "        errors.append('length_issues')\n",
        "\n",
        "    # Check for potential hallucinations (if model summary is much longer)\n",
        "    if len(model_summary.split()) > 1.5*len(human_summary.split()):\n",
        "        errors.append('hallucination')\n",
        "\n",
        "    # Check if key terms from human summary are missing in model summary\n",
        "    human_words = set(word.lower() for word in human_summary.split() if len(word) > 4)\n",
        "    model_words = set(word.lower() for word in model_summary.split() if len(word) > 4)\n",
        "    missing_ratio = len(human_words - model_words) / len(human_words) if human_words else 0\n",
        "\n",
        "    if missing_ratio > 0.4:  # If more than 40% of key words are missing\n",
        "        errors.append('missing_key_info')\n",
        "\n",
        "    # If no overlap in key entities\n",
        "    if missing_ratio > 0.7:\n",
        "        errors.append('focus_problems')\n",
        "\n",
        "    return errors\n",
        "\n",
        "# Analyze each example\n",
        "for index in analysis_indices:\n",
        "    prompt = \"Summarize this conversation:\\n\" + dataset['test'][index]['dialogue']\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "    # Move inputs to GPU if available\n",
        "    if cuda_available:\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate summaries\n",
        "    with torch.no_grad():\n",
        "        baseline_outputs = baseline_model.generate(**inputs, max_new_tokens=50)\n",
        "        finetuned_outputs = instruct_model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "    # Decode outputs\n",
        "    baseline_summary = tokenizer.decode(baseline_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_summary = tokenizer.decode(finetuned_outputs[0], skip_special_tokens=True)\n",
        "    human_summary = dataset['test'][index]['summary']\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    baseline_scores = scorer.score(human_summary, baseline_summary)\n",
        "    finetuned_scores = scorer.score(human_summary, finetuned_summary)\n",
        "\n",
        "    # Categorize errors\n",
        "    baseline_errors = categorize_errors(human_summary, baseline_summary)\n",
        "    finetuned_errors = categorize_errors(human_summary, finetuned_summary)\n",
        "\n",
        "    # Store results\n",
        "    error_analysis_data.append({\n",
        "        'example_id': index,\n",
        "        'dialogue_snippet': dataset['test'][index]['dialogue'][:100] + '...',\n",
        "        'human_summary': human_summary,\n",
        "        'baseline_summary': baseline_summary,\n",
        "        'finetuned_summary': finetuned_summary,\n",
        "        'baseline_rouge1': baseline_scores['rouge1'].fmeasure,\n",
        "        'baseline_rouge2': baseline_scores['rouge2'].fmeasure,\n",
        "        'baseline_rougeL': baseline_scores['rougeL'].fmeasure,\n",
        "        'finetuned_rouge1': finetuned_scores['rouge1'].fmeasure,\n",
        "        'finetuned_rouge2': finetuned_scores['rouge2'].fmeasure,\n",
        "        'finetuned_rougeL': finetuned_scores['rougeL'].fmeasure,\n",
        "        'baseline_errors': baseline_errors,\n",
        "        'finetuned_errors': finetuned_errors\n",
        "    })\n",
        "\n",
        "# Convert to dataframe\n",
        "error_df = pd.DataFrame(error_analysis_data)\n",
        "\n",
        "# Display a few examples with their error categories\n",
        "for i in range(min(5, len(error_df))):\n",
        "    print(dash_line)\n",
        "    print(f\"Example {i+1} (ID: {error_df.iloc[i]['example_id']})\")\n",
        "    print(dash_line)\n",
        "    print(\"Dialogue snippet:\")\n",
        "    print(error_df.iloc[i]['dialogue_snippet'])\n",
        "    print(\"\\nHuman summary:\")\n",
        "    print(error_df.iloc[i]['human_summary'])\n",
        "    print(\"\\nBaseline summary:\")\n",
        "    print(error_df.iloc[i]['baseline_summary'])\n",
        "    print(f\"ROUGE-1: {error_df.iloc[i]['baseline_rouge1']:.4f}\")\n",
        "    print(f\"Error categories: {', '.join([error_categories[e] for e in error_df.iloc[i]['baseline_errors']])}\")\n",
        "    print(\"\\nFine-tuned summary:\")\n",
        "    print(error_df.iloc[i]['finetuned_summary'])\n",
        "    print(f\"ROUGE-1: {error_df.iloc[i]['finetuned_rouge1']:.4f}\")\n",
        "    print(f\"Error categories: {', '.join([error_categories[e] for e in error_df.iloc[i]['finetuned_errors']])}\")\n",
        "    print(dash_line)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tySGF2EOOlY9",
      "metadata": {
        "id": "tySGF2EOOlY9"
      },
      "source": [
        "### Error Analysis Summary\n",
        "\n",
        "Let's analyze the patterns in errors and see how fine-tuning has addressed specific types of errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W1bJGLD1OnPR",
      "metadata": {
        "id": "W1bJGLD1OnPR"
      },
      "outputs": [],
      "source": [
        "# Add matplotlib import at the beginning of your code cell\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Aggregate error statistics\n",
        "baseline_error_counts = defaultdict(int)\n",
        "finetuned_error_counts = defaultdict(int)\n",
        "\n",
        "for _, row in error_df.iterrows():\n",
        "    for error in row['baseline_errors']:\n",
        "        baseline_error_counts[error] += 1\n",
        "    for error in row['finetuned_errors']:\n",
        "        finetuned_error_counts[error] += 1\n",
        "\n",
        "# Create a summary dataframe\n",
        "error_summary = []\n",
        "for error_type in error_categories.keys():\n",
        "    error_summary.append({\n",
        "        'Error Type': error_categories[error_type],\n",
        "        'Baseline Count': baseline_error_counts[error_type],\n",
        "        'Fine-tuned Count': finetuned_error_counts[error_type],\n",
        "        'Improvement': baseline_error_counts[error_type] - finetuned_error_counts[error_type]\n",
        "    })\n",
        "\n",
        "error_summary_df = pd.DataFrame(error_summary)\n",
        "print(\"Error Type Distribution and Improvement:\")\n",
        "print(error_summary_df)\n",
        "\n",
        "# Visualize error distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "error_types = [error_categories[e] for e in error_categories.keys()]\n",
        "baseline_counts = [baseline_error_counts[e] for e in error_categories.keys()]\n",
        "finetuned_counts = [finetuned_error_counts[e] for e in error_categories.keys()]\n",
        "\n",
        "x = np.arange(len(error_types))\n",
        "width = 0.35\n",
        "\n",
        "plt.bar(x - width/2, baseline_counts, width, label='Baseline Model')\n",
        "plt.bar(x + width/2, finetuned_counts, width, label='Fine-tuned Model')\n",
        "\n",
        "plt.xlabel('Error Types')\n",
        "plt.ylabel('Number of Occurrences')\n",
        "plt.title('Distribution of Error Types: Baseline vs. Fine-tuned Model')\n",
        "plt.xticks(x, error_types, rotation=45, ha='right')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('error_analysis.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QV_KoAHCOqi1",
      "metadata": {
        "id": "QV_KoAHCOqi1"
      },
      "source": [
        "## 10. Enhanced Visualizations\n",
        "\n",
        "Let's create more detailed visualizations of our results to better understand the performance improvements and compare model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RlQ2IFF6Or08",
      "metadata": {
        "id": "RlQ2IFF6Or08"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Set a professional style for plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "\n",
        "# 1. ROUGE Score Comparison across the sample examples\n",
        "def create_rouge_comparison_chart(indices):\n",
        "    # Collect the ROUGE scores for baseline and finetuned models\n",
        "    baseline_rouge1_scores = []\n",
        "    baseline_rouge2_scores = []\n",
        "    baseline_rougeL_scores = []\n",
        "    finetuned_rouge1_scores = []\n",
        "    finetuned_rouge2_scores = []\n",
        "    finetuned_rougeL_scores = []\n",
        "\n",
        "    for index in indices:\n",
        "        prompt = \"Summarize this conversation:\\n\" + dataset['test'][index]['dialogue']\n",
        "        inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "        # Move inputs to GPU if available\n",
        "        if cuda_available:\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate summaries\n",
        "        with torch.no_grad():\n",
        "            baseline_outputs = baseline_model.generate(**inputs, max_new_tokens=50)\n",
        "            finetuned_outputs = instruct_model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "        # Decode outputs\n",
        "        baseline_summary = tokenizer.decode(baseline_outputs[0], skip_special_tokens=True)\n",
        "        finetuned_summary = tokenizer.decode(finetuned_outputs[0], skip_special_tokens=True)\n",
        "        human_summary = dataset['test'][index]['summary']\n",
        "\n",
        "        # Calculate ROUGE scores\n",
        "        baseline_scores = scorer.score(human_summary, baseline_summary)\n",
        "        finetuned_scores = scorer.score(human_summary, finetuned_summary)\n",
        "\n",
        "        # Store scores\n",
        "        baseline_rouge1_scores.append(baseline_scores['rouge1'].fmeasure)\n",
        "        baseline_rouge2_scores.append(baseline_scores['rouge2'].fmeasure)\n",
        "        baseline_rougeL_scores.append(baseline_scores['rougeL'].fmeasure)\n",
        "        finetuned_rouge1_scores.append(finetuned_scores['rouge1'].fmeasure)\n",
        "        finetuned_rouge2_scores.append(finetuned_scores['rouge2'].fmeasure)\n",
        "        finetuned_rougeL_scores.append(finetuned_scores['rougeL'].fmeasure)\n",
        "\n",
        "    # Create figure\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # ROUGE-1 comparison\n",
        "    axes[0].bar(range(len(indices)), baseline_rouge1_scores, alpha=0.7, label='Baseline')\n",
        "    axes[0].bar(range(len(indices)), finetuned_rouge1_scores, alpha=0.7, label='Fine-tuned')\n",
        "    axes[0].set_title('ROUGE-1 Comparison')\n",
        "    axes[0].set_xlabel('Example Index')\n",
        "    axes[0].set_ylabel('ROUGE-1 Score')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # ROUGE-2 comparison\n",
        "    axes[1].bar(range(len(indices)), baseline_rouge2_scores, alpha=0.7, label='Baseline')\n",
        "    axes[1].bar(range(len(indices)), finetuned_rouge2_scores, alpha=0.7, label='Fine-tuned')\n",
        "    axes[1].set_title('ROUGE-2 Comparison')\n",
        "    axes[1].set_xlabel('Example Index')\n",
        "    axes[1].set_ylabel('ROUGE-2 Score')\n",
        "    axes[1].legend()\n",
        "\n",
        "    # ROUGE-L comparison\n",
        "    axes[2].bar(range(len(indices)), baseline_rougeL_scores, alpha=0.7, label='Baseline')\n",
        "    axes[2].bar(range(len(indices)), finetuned_rougeL_scores, alpha=0.7, label='Fine-tuned')\n",
        "    axes[2].set_title('ROUGE-L Comparison')\n",
        "    axes[2].set_xlabel('Example Index')\n",
        "    axes[2].set_ylabel('ROUGE-L Score')\n",
        "    axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rouge_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Return scores for further analysis\n",
        "    return {\n",
        "        'baseline_rouge1': baseline_rouge1_scores,\n",
        "        'baseline_rouge2': baseline_rouge2_scores,\n",
        "        'baseline_rougeL': baseline_rougeL_scores,\n",
        "        'finetuned_rouge1': finetuned_rouge1_scores,\n",
        "        'finetuned_rouge2': finetuned_rouge2_scores,\n",
        "        'finetuned_rougeL': finetuned_rougeL_scores\n",
        "    }\n",
        "\n",
        "# Use a different set of examples for visualization\n",
        "vis_indices = random.sample(range(len(dataset['test'])), 10)\n",
        "rouge_scores = create_rouge_comparison_chart(vis_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jQkBpDgaOvWy",
      "metadata": {
        "id": "jQkBpDgaOvWy"
      },
      "outputs": [],
      "source": [
        "# 2. Create a heatmap for ROUGE scores (ROUGE matrix visualization)\n",
        "def create_rouge_matrix(scores):\n",
        "    # Prepare data for the heatmap\n",
        "    # Compute relative improvements\n",
        "    rouge1_improvements = [(f - b) / b * 100 if b > 0 else 0\n",
        "                          for f, b in zip(scores['finetuned_rouge1'], scores['baseline_rouge1'])]\n",
        "    rouge2_improvements = [(f - b) / b * 100 if b > 0 else 0\n",
        "                          for f, b in zip(scores['finetuned_rouge2'], scores['baseline_rouge2'])]\n",
        "    rougeL_improvements = [(f - b) / b * 100 if b > 0 else 0\n",
        "                          for f, b in zip(scores['finetuned_rougeL'], scores['baseline_rougeL'])]\n",
        "\n",
        "    # Combine into a matrix\n",
        "    rouge_matrix = np.array([rouge1_improvements, rouge2_improvements, rougeL_improvements])\n",
        "\n",
        "    # Create heatmap\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.heatmap(rouge_matrix, annot=True, fmt=\".1f\", cmap=\"RdYlGn\",\n",
        "                xticklabels=[f\"Ex {i+1}\" for i in range(len(rouge1_improvements))],\n",
        "                yticklabels=[\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"],\n",
        "                cbar_kws={'label': 'Improvement %'})\n",
        "    plt.title('ROUGE Score Improvements (%) After Fine-tuning')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rouge_matrix.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Now create absolute score heatmaps\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "    # Baseline model scores\n",
        "    baseline_matrix = np.array([scores['baseline_rouge1'], scores['baseline_rouge2'], scores['baseline_rougeL']])\n",
        "    sns.heatmap(baseline_matrix, annot=True, fmt=\".3f\", cmap=\"Blues\", ax=axes[0],\n",
        "                xticklabels=[f\"Ex {i+1}\" for i in range(len(rouge1_improvements))],\n",
        "                yticklabels=[\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"],\n",
        "                cbar_kws={'label': 'Score'})\n",
        "    axes[0].set_title('Baseline Model ROUGE Scores')\n",
        "\n",
        "    # Fine-tuned model scores\n",
        "    finetuned_matrix = np.array([scores['finetuned_rouge1'], scores['finetuned_rouge2'], scores['finetuned_rougeL']])\n",
        "    sns.heatmap(finetuned_matrix, annot=True, fmt=\".3f\", cmap=\"Greens\", ax=axes[1],\n",
        "                xticklabels=[f\"Ex {i+1}\" for i in range(len(rouge1_improvements))],\n",
        "                yticklabels=[\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"],\n",
        "                cbar_kws={'label': 'Score'})\n",
        "    axes[1].set_title('Fine-tuned Model ROUGE Scores')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rouge_scores_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "# Create ROUGE matrix visualization\n",
        "create_rouge_matrix(rouge_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zVCZFcPUOxFR",
      "metadata": {
        "id": "zVCZFcPUOxFR"
      },
      "outputs": [],
      "source": [
        "# 3. Summary length comparison\n",
        "def analyze_summary_length(indices):\n",
        "    human_lengths = []\n",
        "    baseline_lengths = []\n",
        "    finetuned_lengths = []\n",
        "\n",
        "    for index in indices:\n",
        "        prompt = \"Summarize this conversation:\\n\" + dataset['test'][index]['dialogue']\n",
        "        inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "        # Move inputs to GPU if available\n",
        "        if cuda_available:\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate summaries\n",
        "        with torch.no_grad():\n",
        "            baseline_outputs = baseline_model.generate(**inputs, max_new_tokens=50)\n",
        "            finetuned_outputs = instruct_model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "        # Decode outputs\n",
        "        baseline_summary = tokenizer.decode(baseline_outputs[0], skip_special_tokens=True)\n",
        "        finetuned_summary = tokenizer.decode(finetuned_outputs[0], skip_special_tokens=True)\n",
        "        human_summary = dataset['test'][index]['summary']\n",
        "\n",
        "        # Get token counts\n",
        "        human_lengths.append(len(human_summary.split()))\n",
        "        baseline_lengths.append(len(baseline_summary.split()))\n",
        "        finetuned_lengths.append(len(finetuned_summary.split()))\n",
        "\n",
        "    # Create box plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    data = [human_lengths, baseline_lengths, finetuned_lengths]\n",
        "    plt.boxplot(data, labels=['Human', 'Baseline', 'Fine-tuned'])\n",
        "    plt.title('Summary Length Comparison (Word Count)')\n",
        "    plt.ylabel('Word Count')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.savefig('summary_length_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Create scatter plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(human_lengths, baseline_lengths, alpha=0.7, label='Baseline')\n",
        "    plt.scatter(human_lengths, finetuned_lengths, alpha=0.7, label='Fine-tuned')\n",
        "    # Add reference line for perfect length match\n",
        "    max_len = max(max(human_lengths), max(baseline_lengths), max(finetuned_lengths))\n",
        "    plt.plot([0, max_len], [0, max_len], 'k--', alpha=0.5)\n",
        "    plt.xlabel('Human Summary Length (words)')\n",
        "    plt.ylabel('Model Summary Length (words)')\n",
        "    plt.title('Model vs. Human Summary Length Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig('summary_length_scatter.png')\n",
        "    plt.show()\n",
        "\n",
        "# Create length comparison visualizations\n",
        "analyze_summary_length(vis_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F9w4RAZ0OzaZ",
      "metadata": {
        "id": "F9w4RAZ0OzaZ"
      },
      "source": [
        "\n",
        "### Main Discoveries from Error Examination\n",
        "\n",
        "Our analysis of errors reveals multiple behavioral trends in the model:\n",
        "\n",
        "1. **Incomplete Information Capture**: Both model variants occasionally fail to include important dialogue elements, though the fine-tuned variant shows marked improvement.\n",
        "\n",
        "2. **Invented Details**: The baseline variant periodically generates content not found in the original conversation. Fine-tuning reduces this tendency by strengthening task-specific alignment.\n",
        "\n",
        "3. **Comprehension of Context**: Both variants face challenges understanding intricate relationships or subtle meanings within conversations, especially when involving numerous speakers or indirect information.\n",
        "\n",
        "4. **Speaker Attribution Mistakes**: The models may incorrectly assign statements to speakers or muddle participant identities, particularly in prolonged conversations.\n",
        "\n",
        "5. **Central Point Recognition**: The baseline variant occasionally fails to identify the conversation's main objective, instead highlighting less relevant details. The fine-tuned variant performs better at identifying core themes.\n",
        "\n",
        "### Difficult Conversation Categories\n",
        "\n",
        "Our observations indicate the models face particular difficulties with:\n",
        "\n",
        "- **Conversations with multiple participants**: Discussions featuring three or more individuals pose increased summarization complexity.\n",
        "- **Domain-specific language**: Conversations with specialized vocabulary present challenges.\n",
        "- **Lengthy exchanges**: Information from initial segments of extended conversations may be lost.\n",
        "- **Indirect communication**: Important content conveyed through implication rather than direct statement.\n",
        "\n",
        "### Possible Enhancements\n",
        "\n",
        "Our error examination suggests these potential refinements for subsequent development:\n",
        "\n",
        "1. **Specialized training**: Build customized datasets targeting the specific error patterns we've identified.\n",
        "2. **Incremental summarization approach**: For lengthy conversations, create progressive summaries leading to a comprehensive final output.\n",
        "3. **Enhanced speaker differentiation**: Strengthen the model's capacity to distinguish and track individual conversation participants.\n",
        "4. **Comparative learning methods**: Train the model using both high-quality and low-quality summaries of the same conversation for contrast.\n",
        "5. **Performance metric integration**: Utilize automated assessment tools like ROUGE throughout the training phase to steer enhancement efforts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I2yoRQA5O1bB",
      "metadata": {
        "id": "I2yoRQA5O1bB"
      },
      "source": [
        "# Technical Report: Fine-Tuning Flan-T5 for Dialogue Summarization\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "This document outlines our strategy for enhancing conversational summary generation through prompt design and model refinement of the Flan-T5 language model. Conversational summarization represents a crucial natural language processing function applicable to meeting documentation services, customer service automation, and conversational AI platforms. Our research examines how various methodologies can strengthen the model's capacity to produce brief and precise conversation summaries.\n",
        "\n",
        "## 2. Methodology\n",
        "\n",
        "### 2.1 Dataset Selection and Preparation\n",
        "\n",
        "We utilized the DialogSum dataset, comprising 13,460 conversations (10,460 for training, 500 for validation, 2,500 for testing) accompanied by professionally written summaries. This dataset proved particularly appropriate because:\n",
        "\n",
        "- It encompasses varied day-to-day interactions\n",
        "- Conversations maintain reasonable length (averaging 131 words)\n",
        "- Summaries remain brief (averaging 14 words)\n",
        "- The dataset received professional annotation\n",
        "\n",
        "Data preparation required limited preprocessing since the dataset maintained good structure. We standardized each conversation with a uniform instruction format: \"Summarize this conversation:\" preceding the dialogue content.\n",
        "\n",
        "### 2.2 Model Selection\n",
        "\n",
        "We selected the Flan-T5 model based on multiple factors:\n",
        "\n",
        "1. **Instruction optimization**: Flan-T5 underwent instruction optimization across various NLP functions, establishing strong capability for following summarization directives\n",
        "2. **Scalability**: Offered in multiple sizes to accommodate performance and computational needs (we employed Flan-T5-large for inference testing and Flan-T5-base for optimization)\n",
        "3. **Architectural design**: Encoder-decoder structure proves effective for text transformation tasks including summarization\n",
        "4. **Accessibility**: Readily available via the Hugging Face platform\n",
        "\n",
        "### 2.3 Prompt Engineering Approach\n",
        "\n",
        "We investigated a series of prompt design methodologies:\n",
        "\n",
        "1. **Initial baseline**: Raw dialogue input without task instructions\n",
        "2. **Zero-shot with directives**: Incorporating task specification \"Summarize this conversation:\\n\"\n",
        "3. **Zero-shot with structure**: Including format instructions \"\\nSummary: \"\n",
        "4. **Few-shot implementation**: Providing example dialogue-summary combinations preceding the target conversation\n",
        "\n",
        "### 2.4 Fine-Tuning Setup\n",
        "\n",
        "For model optimization, we utilized the Hugging Face Transformers library with this configuration:\n",
        "\n",
        "- Model: Flan-T5-base (balancing effectiveness and computational demands)\n",
        "- Training corpus: Complete DialogSum training collection (10,460 samples)\n",
        "- Validation corpus: DialogSum validation collection (500 samples)\n",
        "- Input structure: \"Summarize this conversation:\\n{dialogue}\"\n",
        "- Output structure: Unaltered summary content\n",
        "- Training platform: PyTorch with GPU acceleration where feasible\n",
        "\n",
        "### 2.5 Hyperparameter Optimization\n",
        "\n",
        "We tested three distinct hyperparameter arrangements:\n",
        "\n",
        "1. **Arrangement 1 (Standard)**:\n",
        "   - Learning rate: 1e-3\n",
        "   - Batch size: 8\n",
        "   - Weight decay: 0.01\n",
        "   - Training cycles: 1\n",
        "\n",
        "2. **Arrangement 2 (Reduced LR, Increased Batch)**:\n",
        "   - Learning rate: 5e-4\n",
        "   - Batch size: 16\n",
        "   - Weight decay: 0.01\n",
        "   - Training cycles: 1\n",
        "\n",
        "3. **Arrangement 3 (Elevated LR, Reduced Weight Decay)**:\n",
        "   - Learning rate: 2e-3\n",
        "   - Batch size: 8\n",
        "   - Weight decay: 0.001\n",
        "   - Training cycles: 1\n",
        "\n",
        "Each arrangement underwent assessment based on training loss, validation loss, and output quality on the test collection.\n",
        "\n",
        "### 2.6 Evaluation Methodology\n",
        "\n",
        "We assessed our methodologies using quantitative and qualitative measurements:\n",
        "\n",
        "**Quantitative Measurements**:\n",
        "- ROUGE-1: Assessing unigram correspondence between produced and reference summaries\n",
        "- ROUGE-2: Assessing bigram correspondence\n",
        "- ROUGE-L: Assessing longest common subsequence\n",
        "\n",
        "**Qualitative Assessment**:\n",
        "- Error classification: Information omission, content fabrication, comprehension failures, etc.\n",
        "- Length analysis: Examining how summary length relates to quality\n",
        "- Key term inclusion: Determining whether critical terms from conversations appear in summaries\n",
        "\n",
        "## 3. Results\n",
        "\n",
        "### 3.1 Prompt Engineering Results\n",
        "\n",
        "Our experiments revealed clear performance progression across different prompting approaches:\n",
        "\n",
        "1. **Initial baseline**: The model frequently extended the conversation instead of summarizing it\n",
        "2. **Zero-shot with directives**: Substantial enhancement, with the model generating actual summaries\n",
        "3. **Zero-shot with structure**: Modest additional gains in summary organization\n",
        "4. **Few-shot implementation**: Additional progress in capturing conversational subtleties\n",
        "\n",
        "### 3.2 Fine-Tuning Results\n",
        "\n",
        "Model optimization yielded considerable gains beyond all prompt design methodologies:\n",
        "\n",
        "- **ROUGE-1**: [X]% enhancement over the optimal prompt design approach\n",
        "- **ROUGE-2**: [X]% enhancement\n",
        "- **ROUGE-L**: [X]% enhancement\n",
        "\n",
        "The optimal hyperparameter arrangement proved to be Arrangement 2 (Learning rate: 5e-4, Batch size: 16), yielding the minimal validation loss and maximum ROUGE measurements.\n",
        "\n",
        "### 3.3 Error Analysis Results\n",
        "\n",
        "Our error examination uncovered multiple critical observations:\n",
        "\n",
        "1. The optimized model demonstrated greatest progress in minimizing \"incomplete key information\" errors\n",
        "2. Content fabrication decreased substantially following optimization\n",
        "3. Both model variants continued struggling with multi-participant conversations and indirect information\n",
        "4. The optimized model generated summaries with length closer to human references\n",
        "\n",
        "## 4. Limitations\n",
        "\n",
        "Despite the advancements, several constraints persist:\n",
        "\n",
        "1. **Domain constraints**: Our model may underperform on specialized conversations (e.g., medical, legal) absent from training data\n",
        "\n",
        "2. **Scale challenges**: Extended conversations (>500 tokens) frequently produce summaries omitting information from initial segments\n",
        "\n",
        "3. **Speaker identification**: The model occasionally misidentifies speaker roles in multi-participant conversations\n",
        "\n",
        "4. **Assessment constraints**: ROUGE measurements, though valuable, inadequately capture semantic equivalence and may overlook correctly phrased alternative summaries\n",
        "\n",
        "5. **Resource demands**: Model optimization demands substantial computational capacity, potentially restricting availability\n",
        "\n",
        "## 5. Future Work\n",
        "\n",
        "Our findings identify multiple promising avenues for subsequent investigation:\n",
        "\n",
        "1. **Staged summarization**: Deploying a multi-phase methodology that initially summarizes conversation portions, then integrates them\n",
        "\n",
        "2. **Speaker-conscious architecture**: Strengthening the model's capability to identify and represent distinct speakers\n",
        "\n",
        "3. **Assessment advancement**: Creating superior automatic measurements capturing summary quality beyond n-gram correspondence\n",
        "\n",
        "4. **Domain customization**: Optimizing on domain-specific conversations for specialized uses\n",
        "\n",
        "5. **Language expansion**: Broadening the methodology to non-English conversations\n",
        "\n",
        "## 6. Conclusion\n",
        "\n",
        "Our research establishes that while prompt design methodologies can meaningfully enhance conversational summary performance, model optimization delivers considerable supplementary advantages. The integration of instruction-based prompting and task-focused optimization produces optimal outcomes, with distinct error reductions across numerous categories.\n",
        "\n",
        "The methodology and discoveries outlined in this document establish groundwork for advancing conversational summarization platforms in real-world applications, while identifying critical domains for subsequent investigation to resolve remaining constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Be0BTCwGO5eG",
      "metadata": {
        "id": "Be0BTCwGO5eG"
      },
      "source": [
        "## 11. Ethical Considerations and Implications\n",
        "\n",
        "Developing and implementing conversational summary generation models requires careful examination of potential ethical ramifications. This section explores critical ethical aspects concerning our research on optimizing Flan-T5 for conversational summarization.\n",
        "\n",
        "### 11.1 Data Representation and Bias\n",
        "\n",
        "**Dataset Characteristics**: The DialogSum dataset, despite its variety, may not uniformly represent all demographic populations, cultural backgrounds, or conversational patterns. This disparity could result in uneven summarization effectiveness across various user groups.\n",
        "\n",
        "**Bias Concerns**: Language models such as Flan-T5 have demonstrated tendencies to inherit and occasionally magnify biases existing in their source data. Our optimized model may reinforce these biases through multiple mechanisms:\n",
        "\n",
        "1. **Contributor emphasis**: The model could emphasize certain participants' input more heavily than others influenced by communication style, assumed expertise, or demographic characteristics linked to language usage.\n",
        "\n",
        "2. **Information prioritization**: The model could consistently favor particular information categories, potentially overlooking culturally relevant context or subtleties.\n",
        "\n",
        "3. **Assessment equity**: Our ROUGE-centered assessment approach might preference summaries matching specific compositional styles rather than alternative equally accurate expressions.\n",
        "\n",
        "**Remediation Approaches**: To counter these issues, subsequent research should:\n",
        "- Examine model effectiveness across varied demographic categories and conversation formats\n",
        "- Include representative examples in optimization data\n",
        "- Create assessment standards acknowledging cultural and compositional variation\n",
        "\n",
        "### 11.2 Misrepresentation and Information Loss\n",
        "\n",
        "**Accuracy Issues**: Summarization naturally involves information reduction, yet concerning patterns of reduction may develop:\n",
        "\n",
        "1. **Distortion**: Our error examination revealed the model occasionally misinterprets context or participant relationships, potentially distorting their intentions or viewpoints.\n",
        "\n",
        "2. **Vital omissions**: In consequential situations (medical, legal, etc.), excluding essential information could produce severe ramifications.\n",
        "\n",
        "3. **Fabrication**: Though diminished following optimization, the model still periodically produces details absent from source conversations, potentially generating inaccurate documentation.\n",
        "\n",
        "**Disclosure Standards**: Any implementation of this technology should:\n",
        "- Explicitly mark summaries as AI-produced\n",
        "- Include reliability metrics or uncertainty signals\n",
        "- Preserve access to source conversations when summaries inform decisions\n",
        "\n",
        "### 11.3 Privacy Considerations\n",
        "\n",
        "**Confidential Data**: Conversations frequently contain confidential information, and models may unintentionally emphasize such details in summaries:\n",
        "\n",
        "1. **Identity markers**: The model could incorporate names, addresses, or other personally identifying details in summaries.\n",
        "\n",
        "2. **Private content**: Personal medical data, financial information, or confidential matters might feature prominently in produced summaries.\n",
        "\n",
        "**Deployment Standards**: Applications utilizing our model should:\n",
        "- Apply privacy-protective preprocessing\n",
        "- Create content screening systems for summaries\n",
        "- Secure proper authorization for summarizing private conversations\n",
        "\n",
        "### 11.4 Deployment Contexts and Power Dynamics\n",
        "\n",
        "**Usage Scenarios**: Various implementation settings present unique ethical concerns:\n",
        "\n",
        "1. **Employment oversight**: Applying conversational summarization for monitoring staff communications could generate surveillance issues and authority disparities.\n",
        "\n",
        "2. **Academic environments**: Summarizing classroom exchanges might affect how learner involvement gets assessed or documented.\n",
        "\n",
        "3. **Client support**: Automated summarization of assistance interactions could influence service standards or portrayal of client issues.\n",
        "\n",
        "**Best Practices**: Organizations implementing this technology should:\n",
        "- Secure informed authorization from all participants in summarized exchanges\n",
        "- Establish procedures to challenge or amend AI-produced summaries\n",
        "- Define explicit guidelines on summary usage and access permissions\n",
        "\n",
        "### 11.5 Environmental Considerations\n",
        "\n",
        "**Processing Demands**: The optimization process for large language models demands substantial processing capacity, presenting environmental consequences:\n",
        "\n",
        "1. **Power usage**: GPU-heavy optimization generates carbon emissions\n",
        "2. **Equipment turnover**: Expedited hardware replacement adds to electronic waste\n",
        "\n",
        "**Performance Enhancements**: Subsequent research should emphasize:\n",
        "- More economical optimization methodologies (e.g., parameter-efficient approaches)\n",
        "- Creating smaller, more efficient models maintaining comparable effectiveness\n",
        "- Measuring and disclosing environmental costs of model training\n",
        "\n",
        "### 11.6 Conclusion: Responsible Development and Deployment\n",
        "\n",
        "Our investigation into conversational summarization carries significant implications for how exchanges get documented, examined, and portrayed. While we've achieved technical advancement in enhancing summarization quality, ethical implementation demands continuous awareness of bias, distortion, confidentiality, authority dynamics, and environmental consequences.\n",
        "\n",
        "We propose that subsequent research in this domain integrate ethical considerations from inception, incorporating varied stakeholder perspectives and systematic auditing of model outputs for potential damages. The objective should be conversational summarization technology that achieves not merely technical competence but also fairness, representation, and value across varied user populations and implementation contexts."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}